---
title: "Performance ranking of flexible species distribution modelling methods does not change with spatial cross-validation. Global Ecology and Biogeography"
description: |
  Appendix S1
author:
  - name: Roozbeh Valavi 
    url: https://github.com/rvalavi
    affiliation: The University of Melbourne, Australia
    affiliation_url: https://ecosystemforest.unimelb.edu.au/
    orcid_id: 0000-0003-2495-5277
  - name: Jane Elith 
    affiliation: The University of Melbourne, Australia
    affiliation_url: https://ecosystemforest.unimelb.edu.au/
    orcid_id: 0000-0002-8706-0326
  - name: José J. Lahoz-Monfort 
    affiliation: The University of Melbourne, Australia
    affiliation_url: https://ecosystemforest.unimelb.edu.au/
    orcid_id: 0000-0002-0845-7035
  - name: Gurutzeta Guillera-Arroita 
    affiliation: The University of Melbourne, Australia
    affiliation_url: https://ecosystemforest.unimelb.edu.au/
    orcid_id: 0000-0002-8387-5739
twitter: 
  site: "@ValaviRoozbeh"
  creator: "@ValaviRoozbeh"
journal: 
  title: "Global Ecology and Biogeography"
  # pasge: "1-27"
  # issn: 2490-1752
  # publisher: ESA
# volume: 44
# issue: 4
# doi: "10.1002/ecm.1486"
date: "`r Sys.Date()`"
# bibliography: references.bib
# bib-humanities: true
output: 
  distill::distill_article:
    toc: true
    toc_depth: '2'
    theme: theme.css
creative_commons: CC BY
header-includes: 
  \usepackage{caption}
  \renewcommand{\figurename}{Fig.} 
  \renewcommand{\thefigure}{S\arabic{figure}} 
  \renewcommand{\thetable}{S\arabic{table}}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE,
	tidy = TRUE,
	# tidy.opts = list(width.cutoff = 80),
	fig.align = "center"
)
```

Appendix S1 for *Valavi, R., Elith, J., Lahos-Monfort, J.J., Guillera-Arroita, G.  (2022) Performance ranking of flexible species distribution modelling methods does not change with spatial cross-validation. Global Ecology and Biogeography.*


## Top rank models in different evaluations

In the main text, the mean performance and the average rank of performance metrics (i.e., $AUC_{ROC}$, $AUC_{PRG}$, and COR) were used to assess and compare models. However, this approach does not show how frequent a model was the top model or whether it was among the top 2-3 models. Here we calculated the percentage of the species (171 species in our study) that a model was in the top 1, top 2, or top 3 models (Figures 1 and 2). For example, for $AUC_{ROC}$ in the random partitioning (Figure 1), the Ensemble model was in the top 3 models  for 63.7% of the species.


```{r fig.height=5, fig.width=6, fig.cap="Top rank models in random partitioning."}
library(tidyverse)

cv_result <- read.csv("results/nceas_blockcv_total.csv")

# changing the names
cv_result$model <- ifelse(cv_result$model == "RF", "RF down-sample", cv_result$model)
cv_result$model <- ifelse(cv_result$model == "Ranger", "RF-shallow", cv_result$model)
cv_result$model <- ifelse(cv_result$model == "GLM", "GLM-step", cv_result$model)
cv_result$model <- ifelse(cv_result$model == "Lasso", "GLM-lasso", cv_result$model)
cv_result$model <- ifelse(cv_result$model == "MaxEnt", "MaxEnt (default)", cv_result$model)

cv_result <- filter(cv_result, !model %in% c("GAM-unweighted", "GLM-unweighted"))
cv_result <- filter(cv_result, !model %in% c("MaxEnt-tuned", "MaxEnt-spatial-tuned", 
                                             "MaxEnt-noclamp", "MaxEnt-H2",
                                             "MaxEnt-LQ", "MaxEnt-LQP"))

morder <- c("SVM",
            "MARS",
            "RF-shallow",
            "GAM",
            "GLM-step",
            "GLM-lasso",
            "BRT",
            "RF down-sample",
            "MaxEnt (default)",
            "Ensemble")

lastn <- function(x, n = 3, by = "COR"){
  x1 <- unique(x[, by, drop = TRUE])
  idx <- tail(order(x1), n)
  xx <- x1[idx]
  idx2 <- which(x[, by, drop = TRUE] %in% xx)
  return(x[idx2, ])
}

topModFun <- function(evalmetric, nm){
  cv_result %>% 
    filter(cv == "random") %>% # filter by cv 
    group_by(species) %>% 
    nest() %>% 
    mutate(df = map(data, ~lastn(x = ., n = nm, by = evalmetric)),
           models = map(df, pluck("model"))) %>% 
    select(models) %>% 
    map(unlist) %>% 
    pluck("models") %>% 
    table() %>% 
    as.data.frame() %>% 
    setNames(c("Model", "freq")) %>% 
    mutate(prop = (freq / 171) * 100,
           percent = round(prop, 1),
           metric = evalmetric,
           position = paste("Top", nm))
}

topModels <- map2(rep(c("ROC", "PRG", "COR"), each = 3), 
                  rep(c(1,2,3), 3), 
                  topModFun) %>% 
  bind_rows()

# create labes for the facet's labeller
topModels$metlabel <- factor(topModels$metric, labels = c(
  'COR',
  '"AUC"["PRG"]',
  '"AUC"["ROC"]'
))
topModels$metlabel <- fct_relevel(topModels$metlabel, c(c('"AUC"["ROC"]',
                                                          '"AUC"["PRG"]',
                                                          'COR')))



# plot all the metrics
ggplot(data = topModels, aes(x = Model, y = forcats::fct_rev(as.factor(position)), fill = percent)) + 
  geom_tile(color = "gray") +
  facet_wrap(vars(metlabel), nrow = 3, strip.position = "right", labeller = label_parsed) +
  geom_text(aes(label = percent, colour = percent), size = 3.5) +
  viridis::scale_fill_viridis(option = "A", direction = -1) +
  viridis::scale_colour_viridis(option = "E", direction = 1, begin = 0.2, end = 0.8) +
  labs(x = NULL, y = NULL) +
  theme_bw() +
  theme(axis.text.x = element_text(size = 8, angle = 60, hjust = 1),
        axis.text.y = element_text(size = 8),
        axis.title.y = element_text(margin = margin(r = 10)),
        text = element_text(size = 11, family = "Helvetica")) +
  guides(fill = "none", colour = "none") +
  scale_x_discrete(limits = morder) +
  ggtitle("Random partitioning")

```

```{r fig.height=5, fig.width=6, fig.cap="Top rank models in spatial partitioning"}

topModFun <- function(evalmetric, nm){
  cv_result %>% 
    filter(cv == "spatial") %>% # filter by cv 
    group_by(species) %>% 
    nest() %>% 
    mutate(df = map(data, ~lastn(x = ., n = nm, by = evalmetric)),
           models = map(df, pluck("model"))) %>% 
    select(models) %>% 
    map(unlist) %>% 
    pluck("models") %>% 
    table() %>% 
    as.data.frame() %>% 
    setNames(c("Model", "freq")) %>% 
    mutate(prop = (freq / 171) * 100,
           percent = round(prop, 1),
           metric = evalmetric,
           position = paste("Top", nm))
}

topModels <- map2(rep(c("ROC", "PRG", "COR"), each = 3), 
                  rep(c(1,2,3), 3), 
                  topModFun) %>% 
  bind_rows()

# create labes for the facet's labeller
topModels$metlabel <- factor(topModels$metric, labels = c(
  'COR',
  '"AUC"["PRG"]',
  '"AUC"["ROC"]'
))
topModels$metlabel <- fct_relevel(topModels$metlabel, c(c('"AUC"["ROC"]',
                                                          '"AUC"["PRG"]',
                                                          'COR')))

# plot all the metrics
ggplot(data = topModels, aes(x = Model, y = forcats::fct_rev(as.factor(position)), fill = percent)) + 
  geom_tile(color = "gray") +
  facet_wrap(vars(metlabel), nrow = 3, strip.position = "right", labeller = label_parsed) +
  geom_text(aes(label = percent, colour = percent), size = 3.5) +
  viridis::scale_fill_viridis(option = "A", direction = -1) +
  viridis::scale_colour_viridis(option = "E", direction = 1, begin = 0.2, end = 0.8) +
  labs(x = NULL, y = NULL) +
  theme_bw() +
  theme(axis.text.x = element_text(size = 8, angle = 60, hjust = 1),
        axis.text.y = element_text(size = 8),
        axis.title.y = element_text(margin = margin(r = 10)),
        text = element_text(size = 11, family = "Helvetica")) +
  guides(fill = "none", colour = "none") +
  scale_x_discrete(limits = morder) +
  ggtitle("Spatial partitioning")

```


Notice that all methods performed the best (top 1) for at least a few species in both random and spatial partitioning. A noticeable result here is that some models like GLM-step, MARS, or RF-shallow are average performers overall, predicting some species better than our top models (for example, GLM-step vs BRT in the top 1 models for random partitioning; or RF-shallow vs RF down-sample or MaxEnt in top 1 model for spatial partitioning). See [Valavi *et al.* (202)](https://esajournals.onlinelibrary.wiley.com/doi/full/10.1002/ecm.1486) for further explanation for the same models fitted on the NCEAS data.
   
Another highlight is that the Ensemble model was no always the top performer for many species, however, it was among the top 3 models for more than half of the species in both partitioning strategies (Figures 1 and 2).
A striking result of Ensemble is that it is performing better than other models when predicting spatially separated testing data. In spatial partitioning, Ensemble was the best model with AUCs and the second-best with COR. While in random partitioning, Ensemble was only the second or third best model. We explored this further to assess how Ensemble is performing compared to its component models in the following section.

## Rank of the Ensemble vs its component models

Here we calculated the same plots but only for the Ensemble model and its component models i.e, GLM-lasso, GAM, MaxEnt (default), BRT, and RF down-sample (Figure 3).  The Ensemble appeared in the top 2 and 3 models for more species than its component in both random and spatial partitioning. For top 1 models, it was only best for $AUC_{ROC}$ (along with RF down-sample) in random partitioning, but the best for both AUCs and the second-best for COR in spatial partitioning. The fact that Ensemble appears better than its component in spatial partitioning might be a piece of evidence that ensembling of tuned models can lead to better generalization.


```{r}
ensmodels <- c(
  "GAM",
  "GLM-lasso",
  "BRT",
  "RF down-sample",
  "MaxEnt (default)",
  "Ensemble"
)
cv_result <- cv_result %>%
  filter(model %in% ensmodels)

topModFun <- function(evalmetric, nm){
  cv_result %>% 
    filter(cv == "random") %>% # filter by cv 
    group_by(species) %>% 
    nest() %>% 
    mutate(df = map(data, ~lastn(x = ., n = nm, by = evalmetric)),
           models = map(df, pluck("model"))) %>% 
    select(models) %>% 
    map(unlist) %>% 
    pluck("models") %>% 
    table() %>% 
    as.data.frame() %>% 
    setNames(c("Model", "freq")) %>% 
    mutate(prop = (freq / 171) * 100,
           percent = round(prop, 1),
           metric = evalmetric,
           position = paste("Top", nm))
}

topModels <- map2(rep(c("ROC", "PRG", "COR"), each = 3), 
                  rep(c(1,2,3), 3), 
                  topModFun) %>% 
  bind_rows()

# create labes for the facet's labeller
topModels$metlabel <- factor(topModels$metric, labels = c(
  'COR',
  '"AUC"["PRG"]',
  '"AUC"["ROC"]'
))
topModels$metlabel <- fct_relevel(topModels$metlabel, c(c('"AUC"["ROC"]',
                                                          '"AUC"["PRG"]',
                                                          'COR')))

# plot all the metrics
p1 <- ggplot(data = topModels, aes(x = Model, y = forcats::fct_rev(as.factor(position)), fill = percent)) + 
  geom_tile(color = "gray") +
  facet_wrap(vars(metlabel), nrow = 3, strip.position = "right", labeller = label_parsed) +
  geom_text(aes(label = percent, colour = percent), size = 3.5) +
  viridis::scale_fill_viridis(option = "A", direction = -1) +
  viridis::scale_colour_viridis(option = "E", direction = 1, begin = 0.2, end = 0.8) +
  labs(x = NULL, y = NULL) +
  theme_bw() +
  theme(axis.text.x = element_text(size = 8, angle = 60, hjust = 1),
        axis.text.y = element_text(size = 8),
        axis.title.y = element_text(margin = margin(r = 10)),
        text = element_text(size = 11, family = "Helvetica")) +
  guides(fill = "none", colour = "none") +
  scale_x_discrete(limits = ensmodels) +
  ggtitle("Random partitioning")

```

```{r}
topModFun <- function(evalmetric, nm){
  cv_result %>% 
    filter(cv == "spatial") %>% # filter by cv 
    group_by(species) %>% 
    nest() %>% 
    mutate(df = map(data, ~lastn(x = ., n = nm, by = evalmetric)),
           models = map(df, pluck("model"))) %>% 
    select(models) %>% 
    map(unlist) %>% 
    pluck("models") %>% 
    table() %>% 
    as.data.frame() %>% 
    setNames(c("Model", "freq")) %>% 
    mutate(prop = (freq / 171) * 100,
           percent = round(prop, 1),
           metric = evalmetric,
           position = paste("Top", nm))
}

topModels <- map2(rep(c("ROC", "PRG", "COR"), each = 3), 
                  rep(c(1,2,3), 3), 
                  topModFun) %>% 
  bind_rows()

# create labes for the facet's labeller
topModels$metlabel <- factor(topModels$metric, labels = c(
  'COR',
  '"AUC"["PRG"]',
  '"AUC"["ROC"]'
))
topModels$metlabel <- fct_relevel(topModels$metlabel, c(c('"AUC"["ROC"]',
                                                          '"AUC"["PRG"]',
                                                          'COR')))

# plot all the metrics
p2 <- ggplot(data = topModels, aes(x = Model, y = forcats::fct_rev(as.factor(position)), fill = percent)) + 
  geom_tile(color = "gray") +
  facet_wrap(vars(metlabel), nrow = 3, strip.position = "right", labeller = label_parsed) +
  geom_text(aes(label = percent, colour = percent), size = 3.5) +
  viridis::scale_fill_viridis(option = "A", direction = -1) +
  viridis::scale_colour_viridis(option = "E", direction = 1, begin = 0.2, end = 0.8) +
  labs(x = NULL, y = NULL) +
  theme_bw() +
  theme(axis.text.x = element_text(size = 8, angle = 60, hjust = 1),
        axis.text.y = element_text(size = 8),
        axis.title.y = element_text(margin = margin(r = 10)),
        text = element_text(size = 11, family = "Helvetica")) +
  guides(fill = "none", colour = "none") +
  scale_x_discrete(limits = ensmodels) +
  ggtitle("Spatial partitioning")

```

```{r fig.height=5, fig.width=7.5, fig.cap="Top rank model among Ensemble and its component models."}
cowplot::plot_grid(p1, p2)

```

To assess whether the Ensemble improves with respect to the best model in the set or not, we did further exploration and realised that in all cases of "Top 1" Ensemble actually outperformed its component (rather than being just as good as the best model in its components). This could be an indication that the ensemble gains by combining “complementary” predictions.

## Difference from average

To further highlight the difference between the performance of models, we calculated the difference between $AUC_{ROC}$ of each model from the average $AUC_{ROC}$ of all models for one single species.


```{r}
library(tidyverse)

cv_result <- read.csv("results/nceas_blockcv_total.csv")

cv_result <- filter(cv_result, !model %in% c("GAM-unweighted", "GLM-unweighted"))

# changing the names
cv_result$model <- ifelse(cv_result$model == "RF", "RF down-sample", cv_result$model)
cv_result$model <- ifelse(cv_result$model == "Ranger", "RF-shallow", cv_result$model)
cv_result$model <- ifelse(cv_result$model == "GLM", "GLM-step", cv_result$model)
cv_result$model <- ifelse(cv_result$model == "Lasso", "GLM-lasso", cv_result$model)
cv_result$model <- ifelse(cv_result$model == "MaxEnt", "MaxEnt (default)", cv_result$model)
cv_result <- filter(cv_result, !model %in% c("MaxEnt-tuned", "MaxEnt-spatial-tuned", 
                                           "MaxEnt-noclamp", "MaxEnt-H2",
                                           "MaxEnt-LQ", "MaxEnt-LQP"))

# ggplot(data = cv_result, aes(x = model, y = ROC)) +
#   geom_violin() + 
#   facet_wrap(~ cv)
morder <- c("SVM",
            "MARS",
            "RF-shallow",
            "GAM",
            "GLM-step",
            "GLM-lasso",
            "BRT",
            "RF down-sample",
            "MaxEnt (default)",
            "Ensemble")

```


```{r, fig.width=8.2, fig.height=4.2, fig.cap="Diffrence from average $AUC_{ROC}$."}
stat_fun <- function(x){
  mn <- mean(x, na.rm = TRUE)
  out <- x - mn
  return(out)
}

diff_mean_rcv <- cv_result %>% 
  filter(cv == "random") %>%
  dplyr::select(species, cv, model, ROC) %>% 
  pivot_wider(names_from = model, values_from = ROC) %>% 
  dplyr::select(-species, -cv) %>% 
  apply(MARGIN = 1, FUN = stat_fun) %>%
  as.data.frame() %>% 
  mutate(model = rownames(.)) %>% 
  pivot_longer(cols = 1:(ncol(.) - 1)) %>% 
  mutate(cv = "Random partitioning")
diff_mean_scv <- cv_result %>% 
  filter(cv == "spatial") %>%
  dplyr::select(species, cv, model, ROC) %>% 
  pivot_wider(names_from = model, values_from = ROC) %>% 
  dplyr::select(-species, -cv) %>% 
  apply(MARGIN = 1, FUN = stat_fun) %>%
  as.data.frame() %>% 
  mutate(model = rownames(.)) %>% 
  pivot_longer(cols = 1:(ncol(.) - 1)) %>% 
  mutate(cv = "Spatial partitioning")

diff_mean <- bind_rows(diff_mean_rcv, diff_mean_scv)

ggplot(data = diff_mean, aes(x = model, y = value, col = value)) +
  geom_jitter(alpha = 0.4, width = 0.3) +
  geom_violin(fill = NA) +
  # viridis::scale_colour_viridis(option = "A", direction = -1) +
  scale_colour_gradientn(colours = RColorBrewer::brewer.pal(11, "Spectral"),
                         limits = c(-0.2, 0.2)) +
  facet_wrap(~ cv) +
  # theme_minimal() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_x_discrete(limits = morder) +
  labs(y = "Difference from average", x = "", col = "Difference")

```

The zero line (dashed line in y-axis) in Figure 4 shows no difference with the mean $AUC_{ROC}$. The positive values show prediction with higher $AUC_{ROC}$ than the average for each model/specie, and vice versa. Each point represents an $AUC_{ROC}$ of a model for one species.    
SVM and MARS were mostly below average, but MaxEnt and Ensemble are mostly above average.

```{r, fig.width=8.2, fig.height=4.2, fig.cap="Difference from the minimum $AUC_{ROC}$."}
stat_fun <- function(x){
  mn <- min(x, na.rm = TRUE)
  out <- x - mn
  return(out)
}

diff_mean_rcv <- cv_result %>% 
  filter(cv == "random") %>%
  dplyr::select(species, cv, model, ROC) %>% 
  pivot_wider(names_from = model, values_from = ROC) %>% 
  dplyr::select(-species, -cv) %>% 
  apply(MARGIN = 1, FUN = stat_fun) %>%
  as.data.frame() %>% 
  mutate(model = rownames(.)) %>% 
  pivot_longer(cols = 1:(ncol(.) - 1)) %>% 
  mutate(cv = "Random partitioning")
diff_mean_scv <- cv_result %>% 
  filter(cv == "spatial") %>%
  dplyr::select(species, cv, model, ROC) %>% 
  pivot_wider(names_from = model, values_from = ROC) %>% 
  dplyr::select(-species, -cv) %>% 
  apply(MARGIN = 1, FUN = stat_fun) %>%
  as.data.frame() %>% 
  mutate(model = rownames(.)) %>% 
  pivot_longer(cols = 1:(ncol(.) - 1)) %>% 
  mutate(cv = "Spatial partitioning")

diff_mean <- bind_rows(diff_mean_rcv, diff_mean_scv)

ggplot(data = diff_mean, aes(x = model, y = value, col = value)) +
  geom_violin() +
  geom_jitter(alpha = 0.1) +
  viridis::scale_colour_viridis(option = "A", direction = -1) +
  facet_wrap(~ cv) +
  # theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_x_discrete(limits = morder) +
  labs(y = "Difference from minimum", x = "", col = "Difference")

```

Figure 5 demonstrate the difference in $AUC_{ROC}$ of each model with the worst model (model with the lowest $AUC_{ROC}$) for each species. higher values means better performance.


## Interactions in BRT and MaxEnt

To assess the impacts of interactions in flexible models, we compare two of our top methods, BRT and MaxEnt, with and without interactions. We implemented a BRT model with a limited tree-complexity of `1` also known as **stump**. The main difference between the BRT-stump and the default BRT (with tree-complexity 1 or 5) is that the default BRT can fit a higher level of interaction between the covariates. The implemented BRT model here (Elith *et al.* 2008) utilized internal cross-validation to fund the right complexity for the model. Thus, by limiting tree-complexity to 1, the model adds more trees to find a similar balance in the fitted model as the BRT with tree-complexity 5.    
MaxEnt is also presented as two variants here, the MaxEnt with forced LQ features and one with LQP features. The main difference between these two is that MaxEnt-LQP accommodates interaction as the product of the linear features.


**The main BRT was modelled with a tree-complexity of 1 (stump) was ... times.**

```{r fig.width=8, fig.height=4, fig.cap="Perfromance of implementation of BRT and MaxEnt with forced limited flexibility."}
# BRT and Maxent second run -----------------------------------------------
cv_result <- read.csv("results/nceas_blockcv_total.csv")
brtmxnt <- read.csv("results/nceas_blockcv_brt_maxent.csv")

# changing the names
cv_result$model <- ifelse(cv_result$model == "RF", "RF down-sample", cv_result$model)
cv_result$model <- ifelse(cv_result$model == "Ranger", "RF-shallow", cv_result$model)
cv_result$model <- ifelse(cv_result$model == "GLM", "GLM-step", cv_result$model)
cv_result$model <- ifelse(cv_result$model == "Lasso", "GLM-lasso", cv_result$model)
cv_result$model <- ifelse(cv_result$model == "MaxEnt", "MaxEnt (default)", cv_result$model)

cv_result <- filter(cv_result, !model %in% c("GAM-unweighted", "GLM-unweighted"))
cv_result <- filter(cv_result, !model %in% c("MaxEnt-tuned", "MaxEnt-spatial-tuned", 
                                             "MaxEnt-noclamp", "MaxEnt-H2",
                                             "MaxEnt-LQP"))

cv_result <- bind_rows(cv_result, brtmxnt)

mean_cv2 <-  cv_result %>%
  group_by(model, cv) %>% 
  summarise(
    ROC_mean = mean(ROC), ROC_se = 1 * (sd(ROC) / sqrt(n())),
    PRG_mean = mean(PRG), PRG_se = 1 * (sd(PRG) / sqrt(n())),
    COR_mean = mean(COR, na.rm = TRUE), COR_se = 1 * (sd(COR, na.rm = TRUE) / sqrt(n()))
  )


cols2 <- c(
  "SVM" = "gray90",
  "Ensemble" = "gray90",
  
  "MARS" = "gray90",
  "GLM-step" = "gray90",
  "GLM-lasso" = "gray90",
  "GAM" = "gray90",
  "MaxEnt (default)" = "gray90",
  "MaxEnt-LQP" = "#BE2207",
  "MaxEnt-LQ" = "#35274A",
  
  "RF-shallow" = "gray90",
  "BRT" = "#046C9A",
  "BRT-stump" = "#0B775E",
  "RF down-sample" = "gray90"
)


mean_cv3 <- filter(mean_cv2, model %in% c("BRT", 
                                          "BRT-stump",
                                          "MaxEnt-LQ",
                                          "MaxEnt-LQP"))
mean_cv2 <- filter(mean_cv2, !model %in% c("BRT", 
                                           "BRT-stump",
                                           "MaxEnt-LQ",
                                           "MaxEnt-LQP"))

ggplot(data = mean_cv2, aes(x = ROC_mean, y = COR_mean, color = model)) +
  scale_color_manual(values = cols2) +
  geom_segment(aes(x = ROC_mean, 
                   y = COR_mean - COR_se, 
                   xend = ROC_mean, 
                   yend = COR_mean + COR_se,
                   colour = model),
               alpha = 0.8, data = mean_cv2) +
  geom_segment(aes(x = ROC_mean - ROC_se, 
                   y = COR_mean, 
                   xend = ROC_mean + ROC_se, 
                   yend = COR_mean, 
                   colour = model), 
               alpha = 0.8, data = mean_cv2) +
  geom_point(size = 2) +
  ggrepel::geom_text_repel(aes(x = ROC_mean, 
                               y = COR_mean,
                               colour = model,
                               label = model),
                           force = 5,
                           data = mean_cv2) +
  geom_segment(aes(x = ROC_mean, 
                   y = COR_mean - COR_se, 
                   xend = ROC_mean, 
                   yend = COR_mean + COR_se),
               colour = "gray50",
               alpha = 0.8, data = mean_cv3) +
  geom_segment(aes(x = ROC_mean - ROC_se, 
                   y = COR_mean, 
                   xend = ROC_mean + ROC_se, 
                   yend = COR_mean), 
               colour = "gray50", 
               alpha = 0.8, data = mean_cv3) +
  geom_point(size = 2, data = mean_cv3, aes(x = ROC_mean, y = COR_mean, color = model)) +
  ggrepel::geom_text_repel(aes(x = ROC_mean, 
                               y = COR_mean,
                               colour = model,
                               label = model),
                           force = 5,
                           data = mean_cv3) +
  facet_wrap(~ paste(str_to_title(cv), "partitioning")) +
  labs(x = expression("AUC"["ROC"]), y = "COR") +
  theme_bw(base_line_size = 0.2) +
  theme(text = element_text(size = 12, family = "Helvetica"),
        legend.position = "none") +
  scale_x_continuous(breaks = seq(from = 0.67, to = 0.75, by = 0.02))


```

There is a small and insignificant difference between the performance of the BRT vs BRT-stump and between MaxEnt-LQP vs MaxEnt-LQ. The MaxEnt-LQP had a lower $AUC_{ROC}$ and COR in both partitioning methods which could be because forced interaction in the model is too complex for some species with a very low number of presence records. On the other hand, BRT performed better than BRT-stump as the model uses cross-validation to regulate the appropriate amount of complexity with different parameter sets. 


## Extraploation in testing blocks

It is useful to know whether extrapolation happens when models are used to predict to spatially separated points. Extrapolation occurs when the testing/predicting sites have environmental values outside of the range of environmental conditions used in the training samples. To measure the amount of extrapolation in testing sites we used Multivariate Environmental Similarity Surface (MESS) introduced by Elith *et al.* (2010). We used the `mess` function in the **dismo** R package for computing the MESS values for the presence-absence of evaluation data, using the training presence-TGB as the reference sites. We used only continuous covariates for this. For more explanation on MESS, see Elith *et al.* (2010).    

In Figure 7, we summed the number of testing points with extrapolation (negative MESS values) for each species. This gives a good sense of whether species from a region enforce extrapolation to the models.


```{r }
library(tidyverse)

messall <- read_csv("results/nceas_blockcv_mess.csv")

```


```{r, fig.width=8, fig.height=4, fig.cap="The sum of extrapolated points of each species in each region."}
messall %>% 
  mutate(cv = paste(str_to_title(cv), "partitioning")) %>% 
ggplot(data = ., aes(x = region, y = extrapolate)) +
  geom_boxplot() +
  geom_jitter(alpha = 0.3) +
  facet_wrap(cv ~ .) +
  # theme_bw() +
  labs(x = "Regions", y = "Number of extraploated points")

```


Figure 8 shows the number of extrapolated sites with spatial partitioning compare to random partitioning. You can see a more extrapolation was enforced in spatial partitioning.

```{r, fig.cap="The sum of the number of extrapolated points in each species in random vs spatial partitioning."}
messall %>% 
  pivot_wider(names_from = cv, values_from = extrapolate) %>% 
ggplot(data = ., aes(y = spatial, x = random, col = region)) +
  geom_point(size = 2.5, alpha = 0.4) +
  geom_abline(slope = 1, intercept = 0, color = "gray") +
  scale_x_continuous(limits = c(0, 43)) +
  scale_y_continuous(limits = c(0, 43)) +
  coord_equal() + 
  theme_classic() +
  labs(x = "Random partitioning", y = "Spatial partitioning", col = "Regions")

```


## Statistical test for random partitioning

Here the statistical test on the differences between models in random partitioning are presented (Figure 9). The plots are $AUC_{ROC}$, $AUC_{PRG}$, and COR from top to bottom, respectively. The number on the top of the x-axis shows the range of the ranks of the models. The rank of each model is indicated by the thin line connected to the scale and the lines (models) that are connected by the horizontal thick line are not statistically different at 0.05 significance level.

```{r fig.width=9, fig.cap="The average rank and statistical difference of the models in random partitioning."}
library(tidyverse)
library(scmamp)

cv_result <- read.csv("results/nceas_blockcv_total.csv")

# changing the names
cv_result$model <- ifelse(cv_result$model == "RF", "RF down-sample", cv_result$model)
cv_result$model <- ifelse(cv_result$model == "Ranger", "RF-shallow", cv_result$model)
cv_result$model <- ifelse(cv_result$model == "GLM", "GLM-step", cv_result$model)
cv_result$model <- ifelse(cv_result$model == "Lasso", "GLM-lasso", cv_result$model)
cv_result$model <- ifelse(cv_result$model == "MaxEnt", "MaxEnt-default", cv_result$model)

cv_result <- filter(cv_result, !model %in% c("GAM-unweighted", "GLM-unweighted"))

cv_result2 <- filter(cv_result, !model %in% c("MaxEnt-tuned", "MaxEnt-spatial-tuned", 
                                              "MaxEnt-noclamp", "MaxEnt-H2",
                                              "MaxEnt-LQ", "MaxEnt-LQP"))

################################
auc_ranks1 <- cv_result2 %>% 
  filter(cv == "random") %>% 
  pivot_wider(id_cols = species, names_from = model, values_from = ROC) %>% 
  dplyr::select(- species) %>% 
  as.matrix()

post_results1 <- postHocTest(
  data = auc_ranks1,
  test = "aligned ranks",
  correct = "shaffer",
  control = NULL,
  use.rank = TRUE
)

colnames(post_results1$summary) <- gsub("F.", "F ", colnames(post_results1$summary))
colnames(post_results1$summary) <- gsub("down.sample", "down-sample", colnames(post_results1$summary))
colnames(post_results1$corrected.pval) <- gsub("F.", "F ", colnames(post_results1$summary))
rownames(post_results1$corrected.pval) <- gsub("down.sample", "down-sample", colnames(post_results1$summary))
colnames(post_results1$corrected.pval) <- gsub("F.", "F ", colnames(post_results1$summary))
rownames(post_results1$corrected.pval) <- gsub("down.sample", "down-sample", colnames(post_results1$summary))

################################
auc_ranks2 <- cv_result2 %>% 
  filter(cv == "random") %>% 
  pivot_wider(id_cols = species, names_from = model, values_from = PRG) %>% 
  dplyr::select(- species) %>% 
  as.matrix()

post_results2 <- postHocTest(
  data = auc_ranks2,
  test = "aligned ranks",
  correct = "shaffer",
  control = NULL,
  use.rank = TRUE
)

colnames(post_results2$summary) <- gsub("F.", "F ", colnames(post_results1$summary))
colnames(post_results2$summary) <- gsub("down.sample", "down-sample", colnames(post_results1$summary))
colnames(post_results2$corrected.pval) <- gsub("F.", "F ", colnames(post_results1$summary))
rownames(post_results2$corrected.pval) <- gsub("down.sample", "down-sample", colnames(post_results1$summary))
colnames(post_results2$corrected.pval) <- gsub("F.", "F ", colnames(post_results1$summary))
rownames(post_results2$corrected.pval) <- gsub("down.sample", "down-sample", colnames(post_results1$summary))

################################
auc_ranks3 <- cv_result2 %>% 
  filter(cv == "random") %>% 
  pivot_wider(id_cols = species, names_from = model, values_from = COR) %>% 
  dplyr::select(- species) %>% 
  as.matrix()

post_results3 <- postHocTest(
  data = auc_ranks3,
  test = "aligned ranks",
  correct = "shaffer",
  control = NULL,
  use.rank = TRUE
)

colnames(post_results3$summary) <- gsub("F.", "F ", colnames(post_results3$summary))
colnames(post_results3$summary) <- gsub("down.sample", "down-sample", colnames(post_results3$summary))
colnames(post_results3$corrected.pval) <- gsub("F.", "F ", colnames(post_results3$summary))
rownames(post_results3$corrected.pval) <- gsub("down.sample", "down-sample", colnames(post_results3$summary))
colnames(post_results3$corrected.pval) <- gsub("F.", "F ", colnames(post_results3$summary))
rownames(post_results3$corrected.pval) <- gsub("down.sample", "down-sample", colnames(post_results3$summary))

################################
# c(bottom, left, top, right)
par(mfrow=c(3,1), mai = c(0, 0, 0, 0))
plotRanking(post_results1$corrected.pval, 
            post_results1$summary,
            alpha = 0.05, 
            cex = 1.3,
            decreasing = FALSE)

plotRanking(post_results2$corrected.pval, 
            post_results2$summary,
            alpha = 0.05, 
            cex = 1.3,
            decreasing = FALSE)

plotRanking(post_results3$corrected.pval, 
            post_results3$summary,
            alpha = 0.05, 
            cex = 1.3,
            decreasing = FALSE)

```

<!-- \newpage -->

## Statistical test for MaxEnt models in spatial partitioning

Here the statistical test for MaxEnt models are presented. The Friedman's Aligned Rank test for $AUC_{PRG}$ is not significant indicating no significant difference in MaxEnt variant in this statistics. Thus only the result of pairwise test for $AUC_{ROC}$ and $COR$ are plotted (Figure 10). In general the differences between these models are insignificant.


```{r}
library(tidyverse)
library(scmamp)

cv_result <- read.csv("results/nceas_blockcv_total.csv")

cv_result2 <- filter(cv_result, model %in% c("MaxEnt", 
                                             "MaxEnt-tuned", 
                                             "MaxEnt-spatial-tuned", 
                                             "MaxEnt-noclamp", 
                                             "MaxEnt-LQ"))

cv_result2$model <- ifelse(cv_result2$model == "MaxEnt", "MaxEnt (default)", cv_result2$model)

################################
roc_maxents <- cv_result2 %>% 
  filter(cv == "spatial") %>% 
  pivot_wider(id_cols = species, names_from = model, values_from = ROC) %>% 
  dplyr::select(- species) %>% 
  as.matrix()

friedmanAlignedRanksTest(roc_maxents)

post_results1 <- postHocTest(
  data = roc_maxents,
  test = "aligned ranks",
  correct = "shaffer",
  control = NULL,
  use.rank = TRUE
)


################################
prg_maxents <- cv_result2 %>% 
  filter(cv == "spatial") %>% 
  pivot_wider(id_cols = species, names_from = model, values_from = PRG) %>% 
  dplyr::select(- species) %>% 
  as.matrix()

friedmanAlignedRanksTest(prg_maxents)

post_results2 <- postHocTest(
  data = prg_maxents,
  test = "aligned ranks",
  correct = "shaffer",
  control = NULL,
  use.rank = TRUE
)


################################
cor_maxents <- cv_result2 %>% 
  filter(cv == "spatial") %>% 
  pivot_wider(id_cols = species, names_from = model, values_from = COR) %>% 
  dplyr::select(- species) %>% 
  as.matrix()

friedmanAlignedRanksTest(cor_maxents)

post_results3 <- postHocTest(
  data = cor_maxents,
  test = "aligned ranks",
  correct = "shaffer",
  control = NULL,
  use.rank = TRUE
)

```
    
    
The plots in Figure 10 are $AUC_{ROC}$ (top) and COR (bottom). The number on the top of the x-axis shows the range of the ranks of the models. The rank of each model is indicated by the thin line connected to the scale and the lines (models) that are connected by the horizontal thick line are not statistically different at 0.05 significance level.

```{r fig.cap="The average rank and statistical difference of the MaxEnt models in spatial partitioning."}
# c(bottom, left, top, right)
par(mfrow=c(3,1), mai = c(0, 0, 0, 0))
plotRanking(post_results1$corrected.pval, 
            post_results1$summary,
            alpha = 0.05, 
            cex = 1.3,
            decreasing = FALSE)
# 
# plotRanking(post_results2$corrected.pval, 
#             post_results2$summary,
#             alpha = 0.05, 
#             cex = 1.3,
#             decreasing = FALSE)

plotRanking(post_results3$corrected.pval, 
            post_results3$summary,
            alpha = 0.05, 
            cex = 1.3,
            decreasing = FALSE)
```


## Modelling methods parameters

A summary of model implementation settings are presented in Table 1. The “parameters” column shows model arguments in R programming that are selected in the modelling process. The “value” column shows the value or ranges of values selected for model fitting and tuning for each R function. The “weighted” column specifies if weighting was used. With the weights, background points were down-weighted to have a total (summed) weight equal to the total number of the presences (Valavi *et al.* 2022). All models are fitted in R v4.0.0.    
Some of the models accept weights. GAM, GLMs, and BRT use case weights (i.e., there is a weight for each training sample), and SVM utilizes class weights (i.e., there is a weight for each class; here presence and background samples are the two classes so there are only two weights). The weights are generated by giving a weight of 1 to every presence point and giving the weights to the background in a way that the sum of the weights for the presence and background are equal. For class weights in SVM, an inverse proportional weight was used.


```{r}
knitr::kable(read.csv("model_parameters.csv"), caption = "Parameters used for implementing different modes.")

```
* GLM-step and GLM-lasso were fitted allowing linear and quadratic terms only, with no interactions.


## Parameters of MaxEnt variants

```{r}
knitr::kable(read.csv("maxent_parameters.csv"), caption = "Parameters of MaxEnt model in different MaxEnt variants.")

```


## Code and data availability

You can find the data in `disdat` R package and the TGB data in Elith *et al.* (2020). To run the models, you can use the codes and data provide in [OSF]() repository and run the code. However, because you probably have different R package version than the one we used ion our analysis the results will not be reproducible (still will be similar). To make sure reproducibility is maintained we use [Docker technology]() and provided the `Dockerfile` (and pre-built `docker image`) so you can have the same R packages and system as we used in our analysis. A third option would be just use the `revn.lock` file and retrieve the package versions in you local system with `renv` package.

Here we explain how to use Dockers for creating a virtual system to reproduce our results.

### Installation
Docker can be installed on different platforms. You can find the instruction for installing docker on different operating systems visit [docker website](https://docs.docker.com/get-docker/).

### Creating the image
Follow the instruction below to create the docker image with RSudio installed and all the R and system packages required for running our analysis.

The following command are terminal commands.

### Clone this repository
You need to have [`git`](https://git-scm.com/) installed on your system.

```{bash eval=FALSE, echo=TRUE}
git clone https://github.com/rvalavi/SDM-Spatial-Validation.git

```

### Build the docker image

In Linux system you might need to use `sudo` command before `docker` commands. The following command may take a while!  
    
    
```{bash eval=FALSE, echo=TRUE}
cd SDM-Spatial-Validation

docker build -t rvalavi:4.0 .

```

Wait until the build is complete. Then check to see the images is created.

```{bash eval=FALSE, echo=TRUE}
docker images

```

You should see `rvalavi` with TAG 4.0 listed as a docker image.

### Run the docker container

After the image is created, you need to run a container to get access to the RStudio and packages. The docker container is a live instance of the image and you can use and even make change to it. This however will not changes the image. 
Use the following command to run a container. 

```{bash eval=FALSE, echo=TRUE}
docker run --name rstudio -p 8787:8787 -e PASSWORD=123 -e USER=rstudio -d rvalavi:4.0

```

This code has several components:  
`--name`: name of the container  
`-p`: port on which container is running. We use this to connect to rstudio  
`-e USER` and `-e PASSWORD`: user and password for the rstudio server  
`-v`: mapping a directory in the local system to a directory in the container. This will allow you to save the generated files and code in local drive and also access to code/data inside your system.  
`-d`: run the container in the background   
`rvalavi:4.0`: name and tag of the docker image  


### Run RStudio server
Open an Internet browser and go to `localhost:8787` to open RStudio. Use the user and password you specified in the previous step (here user is "rstudio" and password is "123") to open RStudio.

![](rstudio.png)

You can run models by running the `R/nceas_modelling.R` script.




## Covariates of each region

Table **3 to 8** show the environmental covariates used in each region. These are the variables that do not have a pairwise correlation of more than 0.8. You can find a complete list of variables with more details in Elith *et al.* (2020), and in the help of the [`disdat`](https://CRAN.R-project.org/package=disdat) R package.

```{r}
knitr::kable(read.csv("results/AWT_environment.csv"), caption = "Environmental vartiables for AWT region.")

```

```{r}
knitr::kable(read.csv("results/CAN_environment.csv"), caption = "Environmental vartiables for CAN region.")

```

```{r}
knitr::kable(read.csv("results/NSW_environment.csv"), caption = "Environmental vartiables for NSW region.")

```

```{r}
knitr::kable(read.csv("results/NZ_environment.csv"), caption = "Environmental vartiables for NZ region.")

```

```{r}
knitr::kable(read.csv("results/SA_environment.csv"), caption = "Environmental vartiables for SA region.")

```

```{r}
knitr::kable(read.csv("results/SWI_environment.csv"), caption = "Environmental vartiables for SWI region.")

```

## List of species used in modelling

For creating spatial blocks some species did not have enough data to fit and evaluate models. Here, we provide the list of species we used in our study.

```{r}

sp_list <- read.csv("species_list.csv")
names(sp_list)[1] <- ""
knitr::kable(sp_list, caption = "List of species used for modelling. PO is the number of presence-only recods in the tarining dataset, TGBs is the number of Target-Group-Background samples, and Presence/Absence are the number of records in the evaluation dataset.")


```

## References

* Elith, J., Graham, C., Valavi, R., Abegg, M., Bruce, C., Ferrier, S., Ford, A., Guisan, A., Hijmans, R.J., Huettmann, F., Lohmann, L., Loiselle, B., Moritz, C., Overton, J., Peterson, A.T., Phillips, S., Richardson, K., Williams, S., Wiser, S.K., Wohlgemuth, T. & Zimmermann, N.E. (2020) Presence-only and Presence-absence Data for Comparing Species Distribution Modeling Methods. *Biodiversity Informatics*, 15, 69–80.

* Elith, J., Kearney, M. & Phillips, S. (2010) The art of modelling range-shifting species. *Methods in ecology and evolution*, 1, 330–342.

* Elith, J., Leathwick, J.R. & Hastie, T. (2008) A Working Guide to Boosted Regression Trees. *Journal of Animal Ecology*, 77, 802–813.

* Valavi, R., Guillera‐Arroita, G., Lahoz‐Monfort, J.J. & Elith, J. (2022) Predictive performance of presence‐only species distribution models: a benchmark study with reproducible code. *Ecological Monographs*, 92, 1–27.





